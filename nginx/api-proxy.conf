# vLLM API Proxy for Tailscale Routing
# Routes requests to vLLM containers based on active profile
# Default: routes to vllm-qwen3vl-4b-latest
# To change target, update proxy_pass below to match your active container:
#   - vllm-qwen3vl-4b-latest (default)
#   - vllm-qwen3vl-4b-nightly
#   - vllm-qwen3vl-4b-v012
#   - vllm-qwen3vl-4b-v013
#   - vllm-qwen3vl-4b-thinking
#   - vllm-qwen3vl-4b-fork-shared

server {
    listen 80;
    server_name _;

    # Use Docker's internal DNS resolver for dynamic upstream resolution
    resolver 127.0.0.11 valid=30s;

    # Landing page with API documentation (similar to /docs for FastAPI)
    location = / {
        default_type text/html;
        return 200 '<!DOCTYPE html>
<html>
<head>
    <title>vLLM API - Qwen3-VL-4B</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; background: #1a1a2e; color: #eee; }
        h1 { color: #00d4ff; }
        h2 { color: #7b68ee; border-bottom: 1px solid #333; padding-bottom: 10px; }
        code { background: #0d1117; padding: 2px 6px; border-radius: 4px; color: #7ee787; }
        pre { background: #0d1117; padding: 15px; border-radius: 8px; overflow-x: auto; border: 1px solid #333; }
        .endpoint { background: #16213e; padding: 15px; margin: 10px 0; border-radius: 8px; border-left: 4px solid #00d4ff; }
        .method { display: inline-block; padding: 3px 8px; border-radius: 4px; font-weight: bold; margin-right: 10px; }
        .get { background: #238636; }
        .post { background: #1f6feb; }
        a { color: #58a6ff; }
        .status { padding: 5px 10px; border-radius: 4px; }
        .status.ok { background: #238636; }
    </style>
</head>
<body>
    <h1>vLLM API - Qwen3-VL-4B-Instruct</h1>
    <p>OpenAI-compatible vision-language model API</p>
    
    <h2>Quick Links</h2>
    <p><a href="/health">Health Check</a> | <a href="/v1/models">List Models</a> | <a href="/version">Version</a></p>
    
    <h2>API Endpoints</h2>
    
    <div class="endpoint">
        <span class="method post">POST</span><code>/v1/chat/completions</code>
        <p>Main chat endpoint - send messages with text and/or images</p>
    </div>
    
    <div class="endpoint">
        <span class="method post">POST</span><code>/v1/completions</code>
        <p>Text completion endpoint</p>
    </div>
    
    <div class="endpoint">
        <span class="method get">GET</span><code>/v1/models</code>
        <p>List available models</p>
    </div>
    
    <div class="endpoint">
        <span class="method get">GET</span><code>/health</code>
        <p>Health check endpoint</p>
    </div>
    
    <h2>Example: Chat with Image</h2>
    <pre>curl -X POST "http://YOUR_TAILSCALE_IP:8081/v1/chat/completions" \\
  -H "Content-Type: application/json" \\
  -d &apos;{
    "model": "Qwen/Qwen3-VL-4B-Instruct",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "text", "text": "What is in this image?"},
          {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
        ]
      }
    ],
    "max_tokens": 512
  }&apos;</pre>
    
    <h2>Example: Text-only Chat</h2>
    <pre>curl -X POST "http://YOUR_TAILSCALE_IP:8081/v1/chat/completions" \\
  -H "Content-Type: application/json" \\
  -d &apos;{
    "model": "Qwen/Qwen3-VL-4B-Instruct",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 256
  }&apos;</pre>
    
    <h2>Python Client</h2>
    <pre>from openai import OpenAI

client = OpenAI(
    base_url="http://YOUR_TAILSCALE_IP:8081/v1",
    api_key="not-needed"  # vLLM does not require API key
)

response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-4B-Instruct",
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response.choices[0].message.content)</pre>
</body>
</html>';
    }

    # Redirect /docs to landing page (for consistency with tesseract pattern)
    location = /docs {
        return 301 /;
    }

    # All API endpoints proxy to vLLM
    location / {
        proxy_pass http://vllm-qwen3vl-4b-latest:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # For large file uploads (images, documents)
        client_max_body_size 100M;
        
        # Timeouts for potentially long processing
        proxy_read_timeout 600s;
        proxy_connect_timeout 60s;
        proxy_send_timeout 600s;
        
        # CRITICAL: Disable buffering for streaming responses
        # Without this, nginx buffers entire request before forwarding
        proxy_request_buffering off;
        proxy_buffering off;
        
        # Stream responses directly to client
        proxy_http_version 1.1;
        chunked_transfer_encoding on;
        
        # Increase buffer sizes for headers
        proxy_buffer_size 128k;
        proxy_buffers 4 256k;
        proxy_busy_buffers_size 256k;
    }
}
